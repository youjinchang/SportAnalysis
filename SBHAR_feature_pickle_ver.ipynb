{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youjinchang/SportAnalysis/blob/main/SBHAR_feature_pickle_ver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbTd8zR5Mvfr",
        "outputId": "9e282ae0-8076-4a57-d023-ff98ad20b9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "!pip install joblib\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjdrH1v2Mvfu",
        "outputId": "256ca891-bb58-4089-ffcb-db12ba8c7af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n"
          ]
        }
      ],
      "source": [
        "print(pickle.format_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qry_NyxMvfu",
        "outputId": "8a3c0d0d-95b1-4cdf-951c-1225df6b8e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5cebd66486f2\n"
          ]
        }
      ],
      "source": [
        "import socket\n",
        "hostname = socket.gethostname()\n",
        "print(hostname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9CoaqUl5Mvfu",
        "outputId": "bb1a709c-b521-4d01-fff0-db10e48dac2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tsfresh\n",
            "  Downloading tsfresh-0.19.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting stumpy>=1.7.2\n",
            "  Downloading stumpy-1.11.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (1.21.5)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (1.3.5)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (0.5.2)\n",
            "Collecting statsmodels>=0.13\n",
            "  Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from tsfresh) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (1.0.2)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (2.23.0)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 26.6 MB/s \n",
            "\u001b[?25hCollecting matrixprofile<2.0.0,>=1.1.10\n",
            "  Downloading matrixprofile-1.1.10-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (1.4.1)\n",
            "Requirement already satisfied: dask[dataframe]>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (2.12.0)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (4.63.0)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 37.3 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.9.0->tsfresh) (0.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (2.11.3)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (5.4.8)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (5.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (3.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (21.3)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (1.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (57.4.0)\n",
            "Collecting cloudpickle\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (2.1.0)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2022.1.1-py3-none-any.whl (830 kB)\n",
            "\u001b[K     |████████████████████████████████| 830 kB 36.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2022.1.0-py3-none-any.whl (822 kB)\n",
            "\u001b[K     |████████████████████████████████| 822 kB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (1.0.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (7.1.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (2.4.0)\n",
            "  Downloading distributed-2021.12.0-py3-none-any.whl (802 kB)\n",
            "\u001b[K     |████████████████████████████████| 802 kB 13.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n",
            "\u001b[K     |████████████████████████████████| 802 kB 38.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.11.1-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 42.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.11.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 34.8 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.10.0-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 39.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 19.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.9.0-py3-none-any.whl (779 kB)\n",
            "\u001b[K     |████████████████████████████████| 779 kB 27.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.1-py3-none-any.whl (778 kB)\n",
            "\u001b[K     |████████████████████████████████| 778 kB 34.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.0-py3-none-any.whl (776 kB)\n",
            "\u001b[K     |████████████████████████████████| 776 kB 37.8 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 48.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n",
            "\u001b[K     |████████████████████████████████| 766 kB 38.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 41.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 36.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 39.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n",
            "\u001b[K     |████████████████████████████████| 715 kB 31.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n",
            "\u001b[K     |████████████████████████████████| 705 kB 38.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n",
            "\u001b[K     |████████████████████████████████| 699 kB 36.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 35.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n",
            "\u001b[K     |████████████████████████████████| 684 kB 28.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 24.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 37.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 36.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n",
            "\u001b[K     |████████████████████████████████| 672 kB 46.7 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 22.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 17.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n",
            "\u001b[K     |████████████████████████████████| 656 kB 38.1 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.11.2\n",
            "  Downloading protobuf-3.11.2-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 23.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from matrixprofile<2.0.0,>=1.1.10->tsfresh) (3.2.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf==3.11.2->matrixprofile<2.0.0,>=1.1.10->tsfresh) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->matrixprofile<2.0.0,>=1.1.10->tsfresh) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->matrixprofile<2.0.0,>=1.1.10->tsfresh) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->matrixprofile<2.0.0,>=1.1.10->tsfresh) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->matrixprofile<2.0.0,>=1.1.10->tsfresh) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.3->matrixprofile<2.0.0,>=1.1.10->tsfresh) (3.10.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tsfresh) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->tsfresh) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->tsfresh) (1.1.0)\n",
            "Collecting numba>=0.54\n",
            "  Downloading numba-0.55.1-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 32.5 MB/s \n",
            "\u001b[?25hCollecting scipy>=1.2.0\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 136 kB/s \n",
            "\u001b[?25hCollecting llvmlite<0.39,>=0.38.0rc1\n",
            "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.11.0->tsfresh) (1.0.1)\n",
            "Installing collected packages: locket, llvmlite, scipy, protobuf, partd, numba, fsspec, cloudpickle, stumpy, statsmodels, matrixprofile, distributed, tsfresh\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow-metadata 1.7.0 requires protobuf<4,>=3.13, but you have protobuf 3.11.2 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "googleapis-common-protos 1.56.0 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-2.0.0 distributed-2.30.1 fsspec-2022.2.0 llvmlite-0.38.0 locket-0.2.1 matrixprofile-1.1.10 numba-0.55.1 partd-1.2.0 protobuf-3.11.2 scipy-1.7.3 statsmodels-0.13.2 stumpy-1.11.0 tsfresh-0.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle",
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ContextualVersionConflict",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-51f87cccedd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tsfresh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_relevant_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimpute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComprehensiveFCParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tsfresh/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m from tsfresh.convenience.relevant_extraction import (  # noqa: E402\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mextract_relevant_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tsfresh/convenience/relevant_extraction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselect_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from tsfresh.utilities.dataframe_functions import (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tsfresh/feature_extraction/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m from tsfresh.feature_extraction.settings import (\n\u001b[1;32m      7\u001b[0m     \u001b[0mComprehensiveFCParameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tsfresh/feature_extraction/extraction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_calculators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_tsdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtsfresh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComprehensiveFCParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tsfresh/feature_extraction/feature_calculators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatrixprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoSolutionPossible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stumpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0m_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stumpy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;31m# Normalize case for Windows systems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mdist_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormcase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mContextualVersionConflict\u001b[0m: (numba 0.51.2 (/usr/local/lib/python3.7/dist-packages), Requirement.parse('numba>=0.54'), {'stumpy'})"
          ]
        }
      ],
      "source": [
        "!pip install tsfresh\n",
        "from tsfresh import extract_features, extract_relevant_features, select_features\n",
        "from tsfresh.utilities.dataframe_functions import impute\n",
        "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
        "from tsfresh.feature_extraction import settings\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4P471bPMvfv"
      },
      "outputs": [],
      "source": [
        "def MakeDir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aeq-b65Mvfv"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1214)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okm3fauFMvfv"
      },
      "source": [
        "1.2 특징 추출할 X데이터 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuowteGZMvfw"
      },
      "outputs": [],
      "source": [
        "motionname=['WALKING',\n",
        "            'WALKING_UPSTAIRS',\n",
        "            'WALKING_DOWNSTAIRS',\n",
        "            'SITTING',\n",
        "            'STANDING',\n",
        "            'LAYING']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NCw-Hu_Mvfw"
      },
      "outputs": [],
      "source": [
        "loadFromPickle = True\n",
        "nsample_crop = 100\n",
        "n_classes = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaBLjbWoMvfw"
      },
      "outputs": [],
      "source": [
        "# fn_pickle = './pickle/dataset_{}.pickle'.format(nsample_crop)\n",
        "\n",
        "# if loadFromPickle:\n",
        "#     if os.path.exists(fn_pickle):\n",
        "#         print('{} exists....OK'.format(fn_pickle))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmhUngA-Mvfx"
      },
      "outputs": [],
      "source": [
        "fn_pickle = '/home/student4/work/test/dataset_100.pickle'.format(nsample_crop)\n",
        "\n",
        "if loadFromPickle:\n",
        "    if os.path.exists(fn_pickle):\n",
        "        print('{} exists....OK'.format(fn_pickle))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSRo0b36Mvfx"
      },
      "outputs": [],
      "source": [
        "file = open('/home/student4/work/test/dataset_100.pickle'.format(nsample_crop), 'rb')\n",
        "\n",
        "datasave = pickle.load(file)\n",
        "file.close()\n",
        "acc_data = datasave[0]\n",
        "gyro_data = datasave[1]\n",
        "y_true = datasave[2]\n",
        "id_idx = datasave[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vKQjDXZMvfx"
      },
      "outputs": [],
      "source": [
        "idx_to_draw = 800\n",
        "plt.figure(figsize=(4,3))\n",
        "plt.plot(acc_data[idx_to_draw])\n",
        "plt.title('acc_data[{}] #{}:{}'.format(idx_to_draw, y_true[idx_to_draw],motionname[y_true[idx_to_draw]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jI_7E7dMvfx"
      },
      "outputs": [],
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.array(int(Y))\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Em36_TjMvfx"
      },
      "outputs": [],
      "source": [
        "acc_data = np.array(acc_data)\n",
        "gyro_data = np.array(gyro_data)\n",
        "y_true = np.array(y_true)\n",
        "y_true_onehot = []\n",
        "for i in range(len(y_true)):\n",
        "    y_true_onehot.append(convert_to_one_hot(y_true[i]-1,n_classes))\n",
        "y_true_onehot = np.array(y_true_onehot)\n",
        "\n",
        "print(np.shape(acc_data))\n",
        "print(np.shape(gyro_data))\n",
        "print(np.shape(id_idx))\n",
        "print(np.shape(y_true_onehot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSyEKsnPMvfy"
      },
      "outputs": [],
      "source": [
        "data_all = np.concatenate([acc_data, gyro_data], axis=-1)\n",
        "print(np.shape(data_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoBdaKQyMvfy"
      },
      "outputs": [],
      "source": [
        "vdatac = [None] * n_classes\n",
        "vlabelc = [None] * n_classes\n",
        "for i in range(n_classes):\n",
        "    vdata = []\n",
        "    vlabel = []\n",
        "    indices = np.where(y_true == i+1) #y_true 가 해당 동작 번호일때의 index값을 반환\n",
        "    vdata = data_all[indices,:,:]\n",
        "    vlabel = y_true_onehot[indices,:]\n",
        "    vdatac[i] = np.array(vdata)\n",
        "    vdatac[i] = vdatac[i].reshape(vdatac[i].shape[1], vdatac[i].shape[2],vdatac[i].shape[3])\n",
        "    vlabelc[i] = np.array(vlabel)\n",
        "    vlabelc[i] = vlabelc[i].reshape(vlabelc[i].shape[1], vlabelc[i].shape[2])\n",
        "    print('{}\\t{}'.format(vdatac[i].shape, vlabelc[i].shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYimWxgiMvfy"
      },
      "outputs": [],
      "source": [
        "len(np.arange(100)/50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq9qR67_Mvfz"
      },
      "outputs": [],
      "source": [
        "idx_to_draw = 1\n",
        "fig1, axarr = plt.subplots(nrows = n_classes, ncols = 2, figsize=(15, 15),dpi=300)\n",
        "\n",
        "for i in range(n_classes):\n",
        "    for j in range(2):\n",
        "        axarr[i][j].plot(np.arange(100)/50,(vdatac[i][idx_to_draw])[:,j*3:j*3+3])#[j*3:j*3+3])\n",
        "        axarr[i][j].set_title('Activity #{} {} Idx #{}'.format(i+1,motionname[i], idx_to_draw))\n",
        "\n",
        "axarr[-1][0].set_xlabel('sec')\n",
        "axarr[-1][1].set_xlabel('sec')\n",
        "plt.tight_layout(pad=2)\n",
        "\n",
        "#plt.savefig('./fig1.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAv6-4J1Mvfz"
      },
      "outputs": [],
      "source": [
        "for i in range(n_classes):\n",
        "    print(i,'\\t',np.shape(vdatac[i])[0],\n",
        "          '\\t',round(np.shape(vdatac[i])[0]*nsample_crop*20/1000/60,1),\n",
        "          'min /',round(np.shape(vdatac[i])[0]*nsample_crop*20/1000/60,1),\n",
        "          'min/',round(np.shape(vdatac[i])[0]*nsample_crop*20/1000/60/60, 2),\n",
        "          'hrs','\\t',motionname[i])\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wK33XiNMvfz"
      },
      "outputs": [],
      "source": [
        "n_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYALb7wGMvfz"
      },
      "outputs": [],
      "source": [
        "datac_all=[]\n",
        "label_all=[]\n",
        "num = 0\n",
        "for i in range(n_classes):\n",
        "    num += np.shape(vdatac[i])[0]\n",
        "    num2 = np.shape(vdatac[i])[0]\n",
        "    #print(num-num2, num)\n",
        "    #print(np.shape(id_idx[num-num2:num]))\n",
        "\n",
        "    measurement = vdatac[i]\n",
        "    measurement = np.concatenate([measurement, id_idx[num-num2:num]], axis=-1)\n",
        "\n",
        "    # id 열을 합쳐주는 부분\n",
        "    tmp = measurement.reshape(-1, measurement.shape[-1])\n",
        "    print(i, np.shape(measurement),'-->',np.shape(tmp), 'label=', np.shape(label_all), np.shape(vlabelc[i]))\n",
        "\n",
        "    if i == 0:\n",
        "        datac_all = tmp\n",
        "        label_all = vlabelc[i]\n",
        "    else:\n",
        "        datac_all = np.vstack((datac_all, tmp))\n",
        "        label_all = np.vstack((label_all, vlabelc[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VaYLFz_Mvfz"
      },
      "outputs": [],
      "source": [
        "datac_all.shape, label_all.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz1jeUTlMvf0"
      },
      "outputs": [],
      "source": [
        " label_all[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8acZwGcCMvf0"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(datac_all)\n",
        "df.columns = ['a_x','a_y','a_z','g_x','g_y','g_z', 'id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqKF-oMwMvf0"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqcApsNQMvf0"
      },
      "outputs": [],
      "source": [
        "df_0 = df[df['id']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KqQ1gTOMvf0"
      },
      "outputs": [],
      "source": [
        "df_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w4jvBZjMvf0"
      },
      "outputs": [],
      "source": [
        "idx_to_draw = 0\n",
        "\n",
        "ax_min = np.min(df_0['a_x'])\n",
        "ax_max = np.max(df_0['a_x'])\n",
        "ax_mean = np.mean(df_0['a_x'])\n",
        "ax_median = np.median(df_0['a_x'])\n",
        "ax_std = np.std(df_0['a_x'])\n",
        "ax_skew = df_0['a_x'].skew()\n",
        "#ax_dot = np.dot(df_0['a_x'], df_0['a_x'])\n",
        "#ax_sum = np.sum(np.abs(np.diff(df_0['a_x'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0smCiXJMvf1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(df_0['a_x'])\n",
        "plt.axhline(ax_min, color = 'red')\n",
        "plt.axhline(ax_max, color = 'green')\n",
        "plt.axhline(ax_mean, color = 'purple')\n",
        "plt.axhline(ax_median, color = 'blue')\n",
        "plt.axhline(ax_std, color = 'orange')\n",
        "plt.axhline(ax_skew, color = 'black')\n",
        "\n",
        "plt.title('acc. ax')\n",
        "plt.xlabel('sample')\n",
        "plt.ylabel('feature value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EJuY-_WMvf1"
      },
      "outputs": [],
      "source": [
        "df.shape[0] // nsample_crop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlFt8KhXMvf1"
      },
      "source": [
        "1.2.1 Y값 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2iuc-1dMvf1"
      },
      "outputs": [],
      "source": [
        "#y를 sereis형태로 바꾸기\n",
        "Y = pd.Series(np.argmax(label_all, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTFD0w0EMvf1"
      },
      "outputs": [],
      "source": [
        "Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccTu5FYXMvf2"
      },
      "outputs": [],
      "source": [
        "Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59MrMC85Mvf2"
      },
      "source": [
        "*1.3* Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yPvPnLkMvf2"
      },
      "outputs": [],
      "source": [
        "settings_idx = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daKR7BuJMvf3"
      },
      "outputs": [],
      "source": [
        "settings_arr = ['DecisionTree', 'RandomForest', 'SVM', 'NeuralNetwork',\n",
        "                'KNN','NaiveBayes']\n",
        "\n",
        "settings_name = settings_arr[settings_idx]\n",
        "print(settings_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5te0QMNVMvf4"
      },
      "outputs": [],
      "source": [
        "if settings_idx==0: # custom 으로 선택한 feature 들 추출\n",
        "    settings1 = {\n",
        "            'mean': None,\n",
        "            'median': None,\n",
        "            'maximum': None,\n",
        "            'minimum': None,\n",
        "            'standard_deviation': None,\n",
        "            'skewness':None,\n",
        "            'kurtosis':None,\n",
        "            'number_peaks': [{'n': 1}, {'n': 3}, {'n': 5}, {'n': 10}, {'n':50}],\n",
        "            'abs_energy':None,\n",
        "            'absolute_sum_of_changes':None,\n",
        "            'mean_abs_change':None,\n",
        "            'ar_coefficient': [{'coeff': 0, 'k': 10}, {'coeff': 1, 'k': 10},\n",
        "                               {'coeff': 2, 'k': 10}, {'coeff': 3, 'k': 10},\n",
        "                               {'coeff': 4, 'k': 10}, {'coeff': 5, 'k': 10},\n",
        "                               {'coeff': 6, 'k': 10}, {'coeff': 7, 'k': 10},\n",
        "                               {'coeff': 8, 'k': 10}, {'coeff': 9, 'k': 10},\n",
        "                               {'coeff': 10, 'k': 10}],}\n",
        "\n",
        "elif settings_idx==1:\n",
        "    settings1 = settings.MinimalFCParameters() # 9개의 mini feature들만 추출\n",
        "\n",
        "elif settings_idx==2:\n",
        "    settings1 = settings.ComprehensiveFCParameters() # 추출가능한 모든feature 추출\n",
        "\n",
        "elif settings_idx==3:\n",
        "    settings1 = settings.EfficientFCParameters() # Comprehensive feature들 중 계산비용이 오래걸리는 feature 제외하고 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTHuDV7OMvf4"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "loadFromPickle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u597DbtpMvf4"
      },
      "outputs": [],
      "source": [
        "out_pickle_dir = './pickle'\n",
        "fn_pickle = '{}/feature_setting{}_class{}_len{}_raw.pickle'.format(out_pickle_dir,settings_idx, n_classes, nsample_crop)\n",
        "print(fn_pickle)\n",
        "if loadFromPickle:\n",
        "    if os.path.exists(fn_pickle):\n",
        "        print('{} exists....OK'.format(fn_pickle))\n",
        "    else:\n",
        "        print('{} does NOT exists....'.format(fn_pickle))\n",
        "        loadFromPickle = False\n",
        "        print('loadFromPickle....{}'.format(loadFromPickle))\n",
        "        print('System will load data from RAW files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ekq5kBBMvf5"
      },
      "outputs": [],
      "source": [
        "if not loadFromPickle:\n",
        "    X = extract_features(df, column_id='id',\n",
        "                default_fc_parameters=settings1, impute_function=impute)\n",
        "    datasave = [None] * 2\n",
        "    datasave[0] = X\n",
        "    datasave[1] = Y\n",
        "    file = open(fn_pickle, 'wb')\n",
        "    pickle.dump(datasave, file)\n",
        "    file.close()\n",
        "\n",
        "else:\n",
        "    print('load from pickle files')\n",
        "    file = open(fn_pickle, 'rb')\n",
        "    data = pickle.load(file)\n",
        "    file.close()\n",
        "    X = data[0]\n",
        "    Y = data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2XevPJyMvf5"
      },
      "outputs": [],
      "source": [
        "X.shape, Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkL-FrriMvf5"
      },
      "outputs": [],
      "source": [
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESECZC2AMvf5"
      },
      "source": [
        "**1.4 Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDY2Z9f7Mvf6"
      },
      "outputs": [],
      "source": [
        "X_filtered = select_features(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ez4O0IIMvf6"
      },
      "outputs": [],
      "source": [
        "X_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfLXHVCoMvf6"
      },
      "outputs": [],
      "source": [
        "from tsfresh.feature_selection.relevance import calculate_relevance_table\n",
        "p_val = calculate_relevance_table(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce4hDbBnMvf6"
      },
      "outputs": [],
      "source": [
        "p_val.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0oHczmTMvf6"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgFrW0D5Mvf7"
      },
      "outputs": [],
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0y7-FQmMvf7"
      },
      "source": [
        "# 1.5 Classifcation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jqXzptgMvf7"
      },
      "outputs": [],
      "source": [
        "classifiers = ['DecisionTree', 'RandomForest', 'SVM', 'NeuralNetwork', 'KNN','NaiveBayes']\n",
        "classifiers_idx = 0\n",
        "classifier_name = classifiers[classifiers_idx]\n",
        "title = '{}'.format(classifier_name)\n",
        "\n",
        "print(classifier_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukwxJpT3Mvf7"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOsux63pMvf7"
      },
      "outputs": [],
      "source": [
        "if classifiers_idx==0:\n",
        "    clf = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=5))\n",
        "\n",
        "elif classifiers_idx==1:\n",
        "    #RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    clf = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=5))\n",
        "\n",
        "elif classifiers_idx==2:\n",
        "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "\n",
        "elif classifiers_idx==4:\n",
        "    clf = KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "elif classifiers_idx==5:\n",
        "    clf = GaussianNB()\n",
        "\n",
        "elif classifiers_idx==3:\n",
        "    clf = models.Sequential()\n",
        "    clf.add(layers.Dense(128, activation='relu', input_shape=[X_train.shape[1]]))\n",
        "    clf.add(layers.Dense(64, activation='relu'))\n",
        "    clf.add(layers.Dense(32, activation='relu'))\n",
        "    clf.add(layers.Dense(6, activation='sigmoid'))\n",
        "    clf.summary()\n",
        "\n",
        "    clf.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-0qvt4dMvf7"
      },
      "outputs": [],
      "source": [
        "to_Train = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKCYqND4Mvf7"
      },
      "outputs": [],
      "source": [
        "if to_Train:\n",
        "    print(classifier_name)\n",
        "    model_outdir = './out'\n",
        "    outDir = \"{}/feature_{}_setting{}_class{}_len{}\".\n",
        "    format(model_outdir,classifier_name,settings_idx, n_classes, nsample_crop)\n",
        "    MakeDir(outDir)\n",
        "\n",
        "    if classifiers_idx != 3:\n",
        "        clf.fit(X_train, y_train)\n",
        "        model = \"{}/{}_feature_setting{}_class{}_len{}.joblib\".\n",
        "        format(outDir,classifier_name,settings_idx, n_classes, nsample_crop)\n",
        "        joblib.dump(clf, model)\n",
        "\n",
        "\n",
        "    else:\n",
        "        y_train = np.array(y_train)\n",
        "        y_train_onehot = []\n",
        "        for i in range(len(y_train)):\n",
        "            y = convert_to_one_hot(y_train[i], 6)\n",
        "            y_train_onehot.append(y)\n",
        "\n",
        "            fnmodel = outDir+'/model_NN_class{}_len{}.h5'.format(n_classes,nsample_crop)\n",
        "            #scaler = StandardScaler()\n",
        "            #X_train_scaled = scaler.fit(X_train).transform(X_train)\n",
        "            history = clf.fit(X_train, np.array(y_train_onehot),\n",
        "                              batch_size=64 ,epochs=100) #validation_split=0.3\n",
        "            clf.save(fnmodel)\n",
        "\n",
        "else:\n",
        "    model_outdir = './out'\n",
        "    outDir = \"{}/feature_{}_setting{}_class{}_len{}\".format(model_outdir,classifier_name,settings_idx, n_classes, nsample_crop)\n",
        "    print('load model files...')\n",
        "    if classifiers_idx != 3:\n",
        "        model = \"{}/{}_feature_setting{}_class{}_len{}.joblib\".format(outDir,classifier_name,settings_idx, n_classes, nsample_crop)\n",
        "        print(model)\n",
        "        clf = joblib.load(model)\n",
        "    else:\n",
        "        fnmodel = outDir+'/model_NN_class{}_len{}.h5'.format(n_classes, nsample_crop)\n",
        "        clf = load_model(fnmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgWDTP-rMvf7"
      },
      "outputs": [],
      "source": [
        "if classifiers_idx == 3:\n",
        "    #X_test_scaled = scaler.fit(X_test).transform(X_test)\n",
        "    expected_test = clf.predict(X_test)\n",
        "    expected_train = clf.predict(X_train)\n",
        "else:\n",
        "    expected_test = clf.predict(X_test)\n",
        "    expected_train = clf.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhWy9B2zMvf8"
      },
      "outputs": [],
      "source": [
        "if classifiers_idx == 3:\n",
        "    expected_train = np.argmax(expected_train, axis=1)\n",
        "    expected_test = np.argmax(expected_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjtwVelYMvf8"
      },
      "outputs": [],
      "source": [
        "print(classifier_name, 'Test Result')\n",
        "print(classification_report(y_test, expected_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBYnv7uYMvf8"
      },
      "outputs": [],
      "source": [
        "print(classifier_name, 'Train Result')\n",
        "print(classification_report(y_train, expected_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04DSzEdlMvf8"
      },
      "outputs": [],
      "source": [
        "#Return the mean accuracy\n",
        "if classifiers_idx == 3:\n",
        "    y_test = np.array(y_test)\n",
        "    y_test_onehot = []\n",
        "    for i in range(len(y_test)):\n",
        "        y = convert_to_one_hot(y_test[i], 6)\n",
        "        y_test_onehot.append(y)\n",
        "    score0 = clf.evaluate(X_train, np.array(y_train_onehot), verbose=1)\n",
        "    score1 = clf.evaluate(X_test, np.array(y_test_onehot), verbose=1)\n",
        "    score0 = score0[1]\n",
        "    score1 = score1[1]\n",
        "\n",
        "else:\n",
        "    score0 = clf.score(X_train, y_train)\n",
        "    score1 = clf.score(X_test, y_test)\n",
        "\n",
        "print('Train accuracy:',score0)\n",
        "print('Test accuracy:',score1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEjMHvobMvf8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "ff = ['micro', 'macro', 'weighted']\n",
        "\n",
        "f1 = f1_score(y_test, expected_test, average='weighted')\n",
        "f11 = f1_score(y_train, expected_train, average='weighted')\n",
        "print('Train f1-score: ',f11)\n",
        "print('Test f1-score: ',f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJe6Enu7Mvf8"
      },
      "outputs": [],
      "source": [
        "conf_matrix = pd.crosstab(y_test, expected_test) #행(row)이 정답, 열(col)이 맞춘거\n",
        "\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJF0sJAqMvf8"
      },
      "outputs": [],
      "source": [
        "conf_matrix = pd.crosstab(y_train, expected_train) #행(row)이 정답, 열(col) 맞춘거\n",
        "\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpVngdN7Mvf8"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix3x(actual, predicted, classes, title='ConfusionMatrix',\n",
        "                            normalize=False, hide_classname = False,figsize=(4, 4),\n",
        "                            dpi=36, cmap=plt.cm.viridis):\n",
        "\n",
        "    import itertools\n",
        "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "    if not normalize:\n",
        "        conf_matrix = pd.crosstab(actual, predicted)\n",
        "    else:\n",
        "        conf_matrix = pd.crosstab(actual, predicted).apply(lambda r: r / r.sum(), axis=1)\n",
        "\n",
        "\n",
        "    if np.shape(conf_matrix) != (len(classes), len(classes)):\n",
        "        print(np.shape(conf_matrix))\n",
        "        for i in range(len(classes)):\n",
        "            if i not in conf_matrix.columns:\n",
        "                conf_matrix[i] = 0.00\n",
        "        conf_matrix = conf_matrix[[i for i in range(len(classes))]]\n",
        "\n",
        "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    im = ax.imshow(conf_matrix, aspect=1.0, interpolation='nearest',cmap=cmap)\n",
        "\n",
        "    plt.title(title, size=12)\n",
        "\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, aspect=20)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "\n",
        "    if hide_classname:\n",
        "        classes2 = []\n",
        "        for k, _ in enumerate(classes):\n",
        "            classes2.append('C{}'.format(k))\n",
        "        classes = classes2\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "\n",
        "    ax.set_xticks(tick_marks)\n",
        "    ax.set_yticks(tick_marks)\n",
        "    ax.set_yticklabels(classes, fontsize=9)\n",
        "    ax.set_xticklabels(classes, fontsize=9, rotation=90)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = conf_matrix.max() / 2.\n",
        "    thresh = 0.5\n",
        "\n",
        "    for i, j in itertools.product(range(conf_matrix.shape[0]),\n",
        "                                  range(conf_matrix.shape[1])):\n",
        "\n",
        "        if conf_matrix[j][i] > thresh: #thresh[j]:\n",
        "            color=\"white\"\n",
        "\n",
        "        else :\n",
        "            color = \"black\"\n",
        "        ax.text(j, i, format(conf_matrix[j][i], fmt), ha=\"center\", va=\"center\",color=color, fontsize=14)\n",
        "\n",
        "        ax.set_ylabel('Actual')\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.grid(False)\n",
        "\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thl-S68FMvf8"
      },
      "outputs": [],
      "source": [
        "motionname = ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing','Laying']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ2grbR_Mvf8"
      },
      "outputs": [],
      "source": [
        "#fig_conf = plot_confusion_matrix(testy1, pred1, class_name, title='Confusionmatrix - {}'.format(ttl1), normalize=True)\n",
        "\n",
        "fig_conf = plot_confusion_matrix3x(y_test, expected_test, motionname, title='{}\n",
        "                                   Confusion matrix'.format(classifier_name), normalize=True,\n",
        "                                   figsize=(8,8), dpi=150,hide_classname = False,cmap = 'binary')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2qTkqVTMvf8"
      },
      "outputs": [],
      "source": [
        "ttl1fn = '{}_setting{}_class{}'.format(classifier_name, settings_idx,len(motionname))\n",
        "ttl1fn = ttl1fn.replace(' ', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ8kQ_QtMvf8"
      },
      "outputs": [],
      "source": [
        "fn_conf = \"{}/Confusion_matrix_{}_selected_sample180.png\".format(outDir,ttl1fn)#n_labeled_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVHrTMLNMvf8"
      },
      "outputs": [],
      "source": [
        "print(fn_conf)\n",
        "fig_conf.savefig(fn_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g0Ddva8Mvf9"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "\n",
        "if classifiers_idx == 0:\n",
        "\n",
        "    feature_names = X_train.columns\n",
        "    clf = DecisionTreeClassifier(max_depth=5)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    dot_data = export_graphviz(clf, out_file=None, feature_names=feature_names,class_names=motionname,\n",
        "                               class_names=motionname,filled=True, rounded=True,special_characters=True)\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "    dt_img = Image(graph.create_png(), width=600, height=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvz0GRMEMvf9"
      },
      "outputs": [],
      "source": [
        "if classifiers_idx == 0:\n",
        "    display(dt_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1GGD90YMvf9"
      },
      "outputs": [],
      "source": [
        "if classifiers_idx == 3:\n",
        "    from sklearn.manifold import TSNE\n",
        "    emb_model = Model(clf.input, clf.layers[-2].output)\n",
        "    #emb_model.summary()\n",
        "    embedding = emb_model.predict(X_test)\n",
        "    print(embedding.shape)\n",
        "\n",
        "    print(y_test.shape)\n",
        "    bb = y_test\n",
        "    print(bb.shape)\n",
        "\n",
        "    proj = TSNE(n_components=2).fit_transform(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJW98BhsMvf9"
      },
      "outputs": [],
      "source": [
        "if classifiers_idx == 3:\n",
        "    cmp = plt.get_cmap(\"tab20\")\n",
        "\n",
        "    plt.figure(figsize=(6,6), dpi=150)\n",
        "\n",
        "    for i in tqdm(range(n_classes)):\n",
        "        select_flag = bb == i\n",
        "        plt_latent = proj[select_flag, :]\n",
        "        plt.scatter(plt_latent[:, 0], plt_latent[:, 1], color=cmp(i), marker=\".\"\n",
        "\n",
        "    fnfig = \"{}/embedding_{}.png\".format(outDir, ttl1fn)#n_labeled_data\n",
        "    print(fnfig)\n",
        "\n",
        "    plt.title(ttl1fn)\n",
        "    plt.gca().legend(motionname, bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "    plt.savefig(fnfig, bbox_inches = \"tight\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}